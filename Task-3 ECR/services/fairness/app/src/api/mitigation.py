import logging
from typing import Any, Dict, List, Optional

import numpy as np
import pandas as pd
from fastapi import APIRouter, Depends, HTTPException
from pydantic import BaseModel

from services.fairness.app.src.api.bias import (
    _clean_nan_values,
    _encode_dataframe,
    _safe_convert_to_numeric,
)
from services.fairness.app.src.bias_detector import BiasDetector
from services.fairness.app.src.mitigation_strategies import MitigationPipeline, MitigationStrategies
from services.fairness.app.utils.helper_functions import (
    get_s3_file_metadata,
    load_dataframe_from_url,
    load_model_from_url,
    validate_file_metadata,
)
from shared.auth import get_current_user

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/fairness/mitigation", tags=["Mitigation Strategies"])


def _load_and_prepare(
    token: str,
    project_id: str,
    target_column: str,
    prediction_column: Optional[str] = None,
    sensitive_feature_column: Optional[str] = None,
    require_model: bool = False,
):
    """
    Helper: Loads train_df from S3, validates, returns:
    train_df, X, X_encoded, y_true (int array), sensitive_attr (int array or None), model (or None), y_pred
    Raises HTTPException on validation failures.
    Keeping this as a comment for description
    """
    # 1. Get S3 metadata
    metadata = get_s3_file_metadata(token, project_id)
    if not metadata:
        raise HTTPException(status_code=404, detail="Project or files not found")

    # 2. Validate and get URLs
    try:
        train_url, test_url, model_url = validate_file_metadata(metadata)
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

    # 3. Load Training Data
    try:
        train_df = load_dataframe_from_url(train_url)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to load training data: {str(e)}")

    if target_column not in train_df.columns:
        raise HTTPException(status_code=400, detail=f"Target column '{target_column}' not found in training data")

    # Prepare data
    X = train_df.drop(columns=[target_column]).copy()
    y_true = _safe_convert_to_numeric(train_df[target_column].values, "y_true")

    # 4. Load Model (if available or required)
    model = None
    if model_url:
        try:
            model = load_model_from_url(model_url)
            logger.info("Loaded user model for mitigation")
        except Exception as e:
            logger.warning(f"Failed to load user model: {e}")
            if require_model:
                raise HTTPException(status_code=400, detail=f"Failed to load required model: {e}")
    elif require_model:
        raise HTTPException(status_code=400, detail="Model required but not found in S3 metadata")

    sensitive_attr = None
    if sensitive_feature_column:
        if sensitive_feature_column not in train_df.columns:
            raise HTTPException(
                status_code=400,
                detail=f"Sensitive feature column '{sensitive_feature_column}' not found in data. Available columns: {list(train_df.columns)}",
            )
        sensitive_attr = _safe_convert_to_numeric(train_df[sensitive_feature_column].values, sensitive_feature_column)

    # Predictions if requested (prediction_column takes precedence)
    y_pred = None
    if prediction_column and prediction_column.strip():
        if prediction_column not in train_df.columns:
            raise HTTPException(status_code=400, detail=f"Prediction column '{prediction_column}' not found")
        y_pred = _safe_convert_to_numeric(train_df[prediction_column].values, prediction_column)
    elif model is not None:
        X_encoded, _ = _encode_dataframe(X)
        try:
            y_pred = _safe_convert_to_numeric(model.predict(X_encoded), "model_prediction")
        except Exception as e:
            logger.warning(f"Model prediction failed: {e}")
            if prediction_column:  # Should have been caught above, but fallback safety
                pass
            else:
                # If we can't predict and have no column, y_pred remains None
                pass

    X_encoded, _ = _encode_dataframe(X)
    return train_df, X, X_encoded, y_true, sensitive_attr, model, y_pred


class MitigationRequest(BaseModel):
    strategy: str
    sensitive_feature_column: str
    target_column: str
    prediction_column: Optional[str] = None


class StrategyComparisonRequest(BaseModel):
    sensitive_feature_column: str
    target_column: str
    prediction_column: Optional[str] = None
    strategies: Optional[list] = None  # If None, run all


class PipelineRequest(BaseModel):
    sensitive_feature_column: str
    target_column: str
    prediction_column: Optional[str] = None
    pipeline: Dict[
        str, List[str]
    ]  # {"pre": ["reweighing"], "in": ["fairness_regularization"], "post": ["threshold_optimization"]}


class PipelineComparisonRequest(BaseModel):
    sensitive_feature_column: str
    target_column: str
    prediction_column: Optional[str] = None
    pipelines: List[Dict[str, Any]]  # List of pipeline configs with names


class OptimizationRequest(BaseModel):
    sensitive_feature_column: str
    target_column: str
    prediction_column: Optional[str] = None
    method: str  # 'greedy', 'top_k', or 'brute_force'
    max_strategies: Optional[int] = 3  # For greedy
    k: Optional[int] = 5  # For top_k
    max_strategies_per_stage: Optional[int] = 2  # For brute_force


@router.get("/strategies")
async def get_available_strategies(current_user: str = Depends(get_current_user), project_id: str = None):
    """Get all available mitigation strategies"""
    try:
        mitigator = MitigationStrategies()
        strategies = mitigator.get_strategies_info()

        return {"status": "success", "total_strategies": len(strategies), "strategies": strategies}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error getting strategies: {str(e)}")


@router.post("/apply")
async def apply_mitigation_strategy(
    request: MitigationRequest, current_user: str = Depends(get_current_user), project_id: str = None
):
    """
    Apply a specific mitigation strategy and show before/after bias metrics
    Uses _load_and_prepare to centralize loading/validation and enforce consistent encodings.
    """
    token = current_user.get("token")
    if not project_id:
        raise HTTPException(status_code=400, detail="project_id is required")

    try:
        # Centralized load/prepare
        train_df, X_train, X_train_encoded, y_true, sensitive_attr, model, y_pred_original = _load_and_prepare(
            token=token,
            project_id=project_id,
            target_column=request.target_column,
            prediction_column=request.prediction_column,
            sensitive_feature_column=request.sensitive_feature_column,
            require_model=False,  # Mitigation usually requires a model, but some strategies (reweighing) don't
        )

        # If model-based strategies are required, ensure model present
        # Some strategies (reweighing) might not STRICTLY need a model object if we just reweigh data,
        # but the logic often assumes one for 'predict' calls later.
        if model is None:
            # If we loaded data but found no model, and require_model=True wasn't strict enough (it threw exception above),
            # or if we relax it. But for now let's assume we need it or will create a dummy?
            # Actually reweighing only needs X, y, sensitive.
            # But let's keep existing logic:
            pass

        if model is None:
            # Check if strategy requires model? Most do for prediction simulation.
            # Allow proceeding if y_pred_original is present OR strategy relies on data transformation
            if (
                request.strategy not in ["reweighing", "disparate_impact_remover", "data_augmentation"]
                and model is None
            ):
                raise HTTPException(status_code=400, detail="Model required for this mitigation strategy")
            if model is None:
                logger.warning(
                    "No model loaded. Some post-mitigation predictions might fail if strategy doesn't provide them."
                )

        # Fallback for baseline metrics: Use y_true if no predictions available (Dataset Bias check)
        if y_pred_original is None:
            logger.info("No prediction column or model provided. Using ground truth labels to measure dataset bias.")
            y_pred_original = y_true.copy()

        logger.info(
            f"Applying mitigation strategy: {request.strategy} on sensitive feature: {request.sensitive_feature_column}"
        )

        # Baseline bias
        bias_detector = BiasDetector()
        baseline_bias = bias_detector.run_all_metrics(y_true, y_pred_original, sensitive_attr)
        baseline_bias = _clean_nan_values(baseline_bias)
        baseline_biased_count = baseline_bias["summary"]["biased_metrics_count"]
        baseline_bias["summary"]["biased_metrics"]

        # Apply mitigation
        mitigator = MitigationStrategies(user_model=model)
        strategy_result = mitigator.run_mitigation_strategy(
            request.strategy,
            X_train_encoded,
            y_true,
            sensitive_attr,
            X_test=X_train_encoded,
            y_test=y_true,
            y_pred_test=y_pred_original,
        )

        # Ensure strategy returned something meaningful
        if strategy_result.get("status") not in ["success", "partial"]:
            logger.error(f"Strategy {request.strategy} failed: {strategy_result.get('error')}")
            raise HTTPException(status_code=400, detail=f"Strategy failed: {strategy_result.get('error')}")

        # Determine mitigated predictions and/or transformed data
        y_pred_mitigated = None
        X_mitigated = X_train_encoded.copy()

        if getattr(mitigator, "X_transformed", None) is not None:
            X_mitigated = mitigator.X_transformed
            # If transformed has more features than expected, truncate (best-effort)
            expected_cols = X_train_encoded.shape[1]
            if hasattr(X_mitigated, "shape") and X_mitigated.shape[1] > expected_cols:
                X_mitigated = X_mitigated[:, :expected_cols]
            try:
                y_pred_mitigated = mitigator.user_model.predict(X_mitigated)
                y_pred_mitigated = _safe_convert_to_numeric(y_pred_mitigated, "mitigated_prediction")
            except Exception as e:
                logger.warning(f"Predict on transformed X failed, falling back to original X: {e}")
                y_pred_mitigated = mitigator.user_model.predict(X_train_encoded)
                y_pred_mitigated = _safe_convert_to_numeric(y_pred_mitigated, "mitigated_prediction")

        elif getattr(mitigator, "sample_weights", None) is not None:
            # Re-train (or attempt) with sample weights if supported
            try:
                import copy

                model_reweighted = copy.deepcopy(mitigator.user_model)
                if hasattr(model_reweighted, "fit"):
                    # Check if it's ONNX (might look like it has fit but shouldn't be trained)
                    is_onnx = hasattr(model_reweighted, "sess") or "onnx" in str(type(model_reweighted)).lower()

                    if not is_onnx:
                        try:
                            # Verify if supports sample_weight
                            import inspect

                            sig = inspect.signature(model_reweighted.fit)
                            if "sample_weight" in sig.parameters or "kwargs" in sig.parameters:
                                model_reweighted.fit(X_train_encoded, y_true, sample_weight=mitigator.sample_weights)
                                logger.info("Retrained model with sample weights")
                            else:
                                logger.info("Model.fit doesn't accept sample_weight; skipping retrain")
                        except Exception as e:
                            logger.warning(f"Failed to retrain model: {e}")
                    else:
                        logger.info("ONNX/Inference model detected; skipping retrain (cannot fit)")
                else:
                    logger.info("Model has no .fit method; skipping retrain")

            except TypeError:
                logger.info("Model.fit issue; skipping retrain")
                y_pred_mitigated = model_reweighted.predict(X_train_encoded)
                y_pred_mitigated = _safe_convert_to_numeric(y_pred_mitigated, "mitigated_prediction")
            except Exception as e:
                logger.exception("Reweighting failed, using original predictions")
                y_pred_mitigated = y_pred_original

        elif getattr(mitigator, "y_pred_adjusted", None) is not None:
            y_pred_mitigated = _safe_convert_to_numeric(mitigator.y_pred_adjusted, "mitigated_prediction")

        elif getattr(mitigator, "model_modified", False):
            model_to_use = getattr(mitigator, "fine_tuned_model", mitigator.user_model)
            y_pred_mitigated = _safe_convert_to_numeric(model_to_use.predict(X_train_encoded), "mitigated_prediction")

        # Final validation: ensure lengths match
        if y_pred_mitigated is None:
            raise HTTPException(status_code=500, detail="Mitigated predictions could not be obtained")

        # If lengths differ and mitigator provided augmented targets, reconcile them
        y_true_for_metrics = y_true.copy()
        sensitive_for_metrics = sensitive_attr.copy()
        if getattr(mitigator, "y_augmented", None) is not None:
            y_true_for_metrics = np.array(mitigator.y_augmented)
            sensitive_for_metrics = np.array(mitigator.sensitive_augmented)

        if len(y_pred_mitigated) != len(y_true_for_metrics):
            logger.warning(
                f"Size mismatch: pred={len(y_pred_mitigated)} target={len(y_true_for_metrics)} - applying truncation if safe"
            )
            if len(y_pred_mitigated) > len(y_true_for_metrics):
                y_pred_mitigated = y_pred_mitigated[: len(y_true_for_metrics)]
            else:
                raise HTTPException(status_code=400, detail="Mitigated predictions fewer than targets after mitigation")

        mitigated_bias = bias_detector.run_all_metrics(y_true_for_metrics, y_pred_mitigated, sensitive_for_metrics)
        mitigated_bias = _clean_nan_values(mitigated_bias)
        mitigated_biased_count = mitigated_bias["summary"]["biased_metrics_count"]
        mitigated_biased_metrics = mitigated_bias["summary"]["biased_metrics"]

        # ===== STEP 4: Calculate improvements =====
        improvement = baseline_biased_count - mitigated_biased_count
        metric_improvements = _calculate_metric_improvements(baseline_bias, mitigated_bias)

        # Build response
        response = {
            "status": "success",
            "strategy_applied": request.strategy,
            "strategy_info": _serialize_mitigation_result(strategy_result),
            "bias_assessment": {
                "baseline": {
                    "summary": baseline_bias["summary"],
                    "detailed_metrics": _serialize_mitigation_result(
                        {k: v for k, v in baseline_bias.items() if k != "summary"}
                    ),
                },
                "after_mitigation": {
                    "summary": mitigated_bias["summary"],
                    "detailed_metrics": _serialize_mitigation_result(
                        {k: v for k, v in mitigated_bias.items() if k != "summary"}
                    ),
                },
            },
            "improvement_analysis": {
                "baseline_biased_metrics_count": int(baseline_biased_count),
                "mitigated_biased_metrics_count": int(mitigated_biased_count),
                "overall_improvement": int(improvement),
                "improvement_percentage": float(
                    (improvement / baseline_biased_count * 100) if baseline_biased_count > 0 else 0
                ),
                "status": "IMPROVED" if improvement > 0 else ("WORSENED" if improvement < 0 else "NO_CHANGE"),
                "metric_improvements": metric_improvements,
            },
            "recommendations": _generate_mitigation_recommendations(
                improvement, strategy_result.get("type"), mitigated_biased_metrics
            ),
        }

        return response

    except HTTPException:
        raise
    except Exception as e:
        logger.exception("Error applying mitigation")
        raise HTTPException(status_code=500, detail=f"Error applying mitigation: {str(e)}")


@router.post("/compare")
async def compare_strategies(
    request: StrategyComparisonRequest, current_user: str = Depends(get_current_user), project_id: str = None
):
    """
    Compare bias scores before and after applying different strategies
    """
    token = current_user.get("token")
    if not project_id:
        raise HTTPException(status_code=400, detail="project_id is required")

    try:
        # Load & prepare using shared helper
        train_df, X_train, X_train_encoded, y_true, sensitive_attr, model, y_pred = _load_and_prepare(
            token=token,
            project_id=project_id,
            target_column=request.target_column,
            prediction_column=request.prediction_column,
            sensitive_feature_column=request.sensitive_feature_column,
            require_model=True,
        )

        if model is None:
            raise HTTPException(status_code=400, detail="Model required for strategy comparison")

        # Get baseline bias metrics
        bias_detector = BiasDetector()
        baseline_results = bias_detector.run_all_metrics(y_true, y_pred, sensitive_attr)
        baseline_results = _clean_nan_values(baseline_results)

        baseline_biased_count = baseline_results["summary"]["biased_metrics_count"]
        print(f"  Baseline biased metrics: {baseline_biased_count}")

        # Get strategies to apply
        if request.strategies:
            strategies_to_run = request.strategies
        else:
            mitigator = MitigationStrategies(user_model=model)
            strategies_to_run = list(mitigator.strategies.keys())

        print(f"\n[STRATEGIES] Evaluating {len(strategies_to_run)} strategies...")
        print(f"  Strategies: {', '.join(strategies_to_run)}")

        # Apply each strategy and measure bias
        mitigation_results = {"baseline": {"bias_metrics": baseline_results, "strategy": "original (no mitigation)"}}

        X_train = train_df.drop(columns=[request.target_column]).copy()
        X_train_encoded, _ = _encode_dataframe(X_train)

        mitigator = MitigationStrategies(user_model=model)

        for idx, strategy_name in enumerate(strategies_to_run, 1):
            print(f"\n[{idx}/{len(strategies_to_run)}] Evaluating: {strategy_name}")

            try:
                # Apply strategy
                strategy_result = mitigator.run_mitigation_strategy(
                    strategy_name,
                    X_train_encoded,
                    y_true,
                    sensitive_attr,
                    X_test=X_train_encoded,
                    y_test=y_true,
                    y_pred_test=y_pred,
                )

                # For now, assume predictions remain same (in real scenario, would re-predict)
                # In production, you'd retrain the model with mitigated data
                if mitigator.X_transformed is not None:
                    X_tmp = mitigator.X_transformed
                    expected_cols = X_train_encoded.shape[1]
                    if X_tmp.shape[1] > expected_cols:
                        X_tmp = X_tmp[:, :expected_cols]
                    try:
                        y_pred_mitigated = model.predict(X_tmp)
                        y_pred_mitigated = _safe_convert_to_numeric(y_pred_mitigated, "mitigated_prediction")
                    except:
                        y_pred_mitigated = y_pred

                elif mitigator.y_pred_adjusted is not None:
                    y_pred_mitigated = _safe_convert_to_numeric(mitigator.y_pred_adjusted, "mitigated_prediction")

                elif mitigator.sample_weights is not None:
                    # simple reweighted retrain
                    try:
                        import copy

                        model_copy = copy.deepcopy(model)

                        is_onnx = hasattr(model_copy, "sess") or "onnx" in str(type(model_copy)).lower()
                        if hasattr(model_copy, "fit") and not is_onnx:
                            model_copy.fit(X_train_encoded, y_true, sample_weight=mitigator.sample_weights)
                            y_pred_mitigated = model_copy.predict(X_train_encoded)
                            y_pred_mitigated = _safe_convert_to_numeric(y_pred_mitigated, "mitigated_prediction")
                        else:
                            # Cannot retrain ONNX -> Use original predictions
                            y_pred_mitigated = y_pred
                    except:
                        y_pred_mitigated = y_pred

                elif mitigator.model_modified:
                    mdl = getattr(mitigator, "fine_tuned_model", model)
                    y_pred_mitigated = _safe_convert_to_numeric(mdl.predict(X_train_encoded), "mitigated_prediction")

                else:
                    y_pred_mitigated = y_pred

                # Calculate bias metrics after mitigation
                if strategy_result.get("status") == "success":
                    print(f"  âœ… Strategy applied successfully")
                    mitigated_results = bias_detector.run_all_metrics(y_true, y_pred_mitigated, sensitive_attr)
                    mitigated_results = _clean_nan_values(mitigated_results)

                    mitigated_biased_count = mitigated_results["summary"]["biased_metrics_count"]
                    improvement = baseline_biased_count - mitigated_biased_count

                    print(
                        f"  ðŸ“Š Biased metrics: {mitigated_biased_count} (improvement: {'+' if improvement >= 0 else ''}{improvement})"
                    )

                    # Clean strategy result for JSON serialization
                    strategy_result_clean = _serialize_mitigation_result(strategy_result)

                    mitigation_results[strategy_name] = {
                        "strategy_info": strategy_result_clean,
                        "bias_metrics": mitigated_results,
                        "improvement": improvement,
                    }
                else:
                    print(f"  âŒ Strategy failed: {strategy_result.get('error', 'Unknown error')}")
                    mitigation_results[strategy_name] = {
                        "status": "error",
                        "error": str(strategy_result.get("error")),
                        "improvement": 0,
                    }

            except Exception as e:
                print(f"  âŒ Exception: {str(e)}")
                mitigation_results[strategy_name] = {"status": "error", "error": str(e), "improvement": 0}

        # Generate comparison summary
        comparison_summary = _generate_comparison_summary(mitigation_results)

        print(f"\n{'='*60}")
        print(f"COMPARISON SUMMARY")
        print(f"{'='*60}")
        print(f"Baseline biased metrics: {baseline_biased_count}")
        print(f"Strategies evaluated: {len(strategies_to_run)}")
        print(
            f"Best performing: {comparison_summary['best_strategies'][0]['strategy'] if comparison_summary['best_strategies'] else 'None'}"
        )
        print(f"{'='*60}\n")

        return {
            "status": "success",
            "analysis_type": "Mitigation Strategy Comparison",
            "sensitive_feature": request.sensitive_feature_column,
            "total_strategies": len(strategies_to_run),
            "strategies_evaluated": strategies_to_run,
            "baseline_biased_metrics": int(baseline_biased_count),
            "mitigation_results": {k: v for k, v in mitigation_results.items()},
            "comparison_summary": comparison_summary,
        }

    except HTTPException:
        raise
    except Exception as e:
        print(f"[ERROR] Error in compare: {str(e)}")
        import traceback

        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error comparing strategies: {str(e)}")


@router.post("/pipeline/apply")
async def apply_mitigation_pipeline(
    request: PipelineRequest, current_user: str = Depends(get_current_user), project_id: str = None
):
    """
    Apply a multi-stage mitigation pipeline (pre -> in -> post)
    Shows bias metrics before and after the full pipeline
    """
    token = current_user.get("token")
    if not project_id:
        raise HTTPException(status_code=400, detail="project_id is required")

    try:
        print(f"\n{'='*60}")
        print(f"MITIGATION PIPELINE REQUEST")
        print(f"{'='*60}")

        # ADD THIS VALIDATION:
        if request.sensitive_feature_column is None or request.sensitive_feature_column.strip() == "":
            raise HTTPException(status_code=400, detail="sensitive_feature_column is required and cannot be empty")

        if request.sensitive_feature_column == "string":
            raise HTTPException(
                status_code=400,
                detail="sensitive_feature_column cannot be the literal string 'string'. Please provide an actual column name",
            )

        print(f"Request parameters:")

        # Extract strategy list from multi-stage pipeline
        strategies = []
        pipeline_dict = request.pipeline or {}

        for stage in ["pre", "in", "post"]:
            if stage in pipeline_dict and isinstance(pipeline_dict[stage], list):
                strategies.extend(pipeline_dict[stage])

        if not strategies:
            raise HTTPException(status_code=400, detail="Pipeline is empty. Add strategies under pre/in/post stages.")

        print(f"  Final strategy execution order: {strategies}")

        if hasattr(request, "sensitive_feature_column"):
            sensitive_feature = request.sensitive_feature_column
        else:
            raise HTTPException(status_code=400, detail="sensitive_feature_column is required")

        if hasattr(request, "prediction_column"):
            prediction_column = request.prediction_column
        else:
            prediction_column = None

        print(f"  Sensitive feature: {sensitive_feature}")
        print(f"  Prediction column: {prediction_column}")

        # Use shared loader for data inputs
        train_df, X_train, X_train_encoded, y_true, sensitive_attr, model, y_pred = _load_and_prepare(
            token=token,
            project_id=project_id,
            target_column=request.target_column,
            prediction_column=request.prediction_column,
            sensitive_feature_column=request.sensitive_feature_column,
            require_model=True,
        )

        # Calculate BASELINE bias metrics
        print(f"\n[BASELINE] Calculating original bias metrics...")
        bias_detector = BiasDetector()

        # Store original sensitive_attr in detector (for later use)
        bias_detector.sensitive_attr_original = sensitive_attr.copy()

        baseline_bias = bias_detector.run_all_metrics(y_true, y_pred, sensitive_attr)
        baseline_bias = _clean_nan_values(baseline_bias)

        baseline_biased_count = baseline_bias["summary"]["biased_metrics_count"]
        print(f"  Baseline biased metrics: {baseline_biased_count}")

        # Build and execute pipeline

        pipeline = MitigationPipeline()

        # Add strategies to pipeline
        for stage in ["pre", "in", "post"]:
            if stage in request.pipeline:
                for strategy in request.pipeline[stage]:
                    try:
                        pipeline.add_strategy(strategy, stage)
                    except Exception as e:
                        raise HTTPException(
                            status_code=400, detail=f"Error adding strategy '{strategy}' to stage '{stage}': {str(e)}"
                        )

        # Get pipeline summary
        pipeline.get_pipeline_summary()

        if model is None:
            raise HTTPException(status_code=400, detail="Model required for mitigation strategies")

        print(f"\n[PIPELINE SETUP]")
        print(f"  Strategies to apply: {strategies}")
        print(f"  Model type: {type(model).__name__}")

        # Generate mitigated predictions (simplified for demo)
        y_pred_current = y_pred.copy()
        pipeline_stages = []

        # Execute pipeline
        pipeline_results = pipeline.execute_pipeline(
            X_train_encoded, y_true, sensitive_attr, X_test=X_train_encoded, y_test=y_true, y_pred_test=y_pred
        )

        X_train_encoded.copy()
        # Track augmented data when it changes
        y_true_current = y_true.copy()
        sensitive_attr_current = sensitive_attr.copy()
        # Apply each strategy in sequence
        for idx, strategy in enumerate(strategies, 1):
            print(f"\n{'='*60}")
            print(f"[STAGE {idx}/{len(strategies)}] {strategy.upper()}")
            print(f"{'='*60}")
            X_current = X_train_encoded

            try:
                # Verify model exists
                if model is None:
                    print(f"  âš ï¸ WARNING: Model is None, skipping strategy {strategy}")
                    stage_result = {
                        "stage_number": idx,
                        "strategy": strategy,
                        "status": "skipped",
                        "reason": "Model not available",
                    }
                    pipeline_stages.append(stage_result)
                    continue
                # Apply mitigation
                mitigator = MitigationStrategies(user_model=model)
                print(f"  Model type: {type(model).__name__}")

                strategy_result = mitigator.run_mitigation_strategy(
                    strategy,
                    X_current,
                    y_true_current,  # Use current (possibly augmented) y_true
                    sensitive_attr_current,  # Use current (possibly augmented) sensitive_attr
                    X_test=X_train_encoded,
                    y_test=y_true,
                    y_pred_test=y_pred_current,
                )

                # Store stage result
                stage_result = {
                    "stage_number": idx,
                    "strategy": strategy,
                    "status": strategy_result.get("status"),
                    "type": strategy_result.get("type"),
                    "message": strategy_result.get("message", ""),
                }
                pipeline_stages.append(stage_result)

                print(f"  Status: {strategy_result.get('status')}")

                # Extract mitigated predictions based on strategy type and handle size changes
                if mitigator.y_augmented is not None:
                    print(f"  Data was augmented: {len(y_true_current)} â†’ {len(mitigator.y_augmented)}")
                    y_true_current = np.array(mitigator.y_augmented)
                    sensitive_attr_current = np.array(mitigator.sensitive_augmented)

                    # Use augmented X for predictions
                    if hasattr(mitigator, "X_augmented") and mitigator.X_augmented is not None:
                        X_current, _ = _encode_dataframe(pd.DataFrame(mitigator.X_augmented))
                    else:
                        X_current = X_current

                elif mitigator.y_pred_adjusted is not None:
                    print(f"  Using adjusted predictions")
                    y_pred_current = mitigator.y_pred_adjusted

                elif mitigator.X_transformed is not None:
                    X_train_encoded_temp = mitigator.X_transformed

                    # Handle feature mismatch
                    num_original_features = X_train_encoded.shape[1]
                    if X_train_encoded_temp.shape[1] > num_original_features:
                        X_train_encoded_temp = X_train_encoded_temp[:, :num_original_features]

                    X_current = X_train_encoded_temp
                    y_pred_current = model.predict(X_train_encoded_temp)
                    y_pred_current = _safe_convert_to_numeric(y_pred_current, "predictions")

                elif mitigator.model_modified:
                    print(f"  Using fine-tuned model")
                    if hasattr(mitigator, "fine_tuned_model") and mitigator.fine_tuned_model is not None:
                        y_pred_current = mitigator.fine_tuned_model.predict(X_train_encoded)
                    else:
                        y_pred_current = model.predict(X_train_encoded)
                    y_pred_current = _safe_convert_to_numeric(y_pred_current, "predictions")

            except Exception as e:
                print(f"[ERROR] {strategy} failed: {str(e)}")
                import traceback

                traceback.print_exc()
                stage_result = {"stage_number": idx, "strategy": strategy, "status": "error", "error": str(e)}
                pipeline_stages.append(stage_result)
                # Continue to next strategy even if one fails
                continue

        # Final mitigated predictions
        y_pred_mitigated = y_pred_current

        # CRITICAL: Validate sizes match before calculating metrics
        print(f"\n[VALIDATION] Final size check before metrics:")
        print(f"  y_true_current: {len(y_true_current)}")
        print(f"  y_pred_mitigated: {len(y_pred_mitigated)}")
        print(f"  sensitive_attr_current: {len(sensitive_attr_current)}")

        if len(y_pred_mitigated) != len(y_true_current):
            print(f"  âš ï¸ Size mismatch! Truncating predictions to match targets")
            y_pred_mitigated = y_pred_mitigated[: len(y_true_current)]

        # Calculate MITIGATED bias metrics
        print(f"\n[AFTER PIPELINE] Calculating mitigated bias metrics...")
        mitigated_bias = bias_detector.run_all_metrics(y_true_current, y_pred_mitigated, sensitive_attr_current)
        mitigated_bias = _clean_nan_values(mitigated_bias)

        print(f"\n{'='*60}")
        print(f"BIAS ASSESSMENT COMPARISON")
        print(f"{'='*60}")

        # Get summary info
        baseline_summary = baseline_bias.get("summary", {})
        mitigated_summary = mitigated_bias.get("summary", {})

        print(f"\nBaseline Summary:")
        print(f"  Overall Status: {baseline_summary.get('overall_bias_status')}")
        print(f"  Biased Metrics Count: {baseline_summary.get('biased_metrics_count')}")
        print(f"  Biased Metrics: {baseline_summary.get('biased_metrics')}")

        print(f"\nMitigated Summary:")
        print(f"  Overall Status: {mitigated_summary.get('overall_bias_status')}")
        print(f"  Biased Metrics Count: {mitigated_summary.get('biased_metrics_count')}")
        print(f"  Biased Metrics: {mitigated_summary.get('biased_metrics')}")

        baseline_biased_count = baseline_summary.get("biased_metrics_count", 0)
        mitigated_biased_count = mitigated_bias["summary"]["biased_metrics_count"]
        improvement = baseline_biased_count - mitigated_biased_count

        print(f"\nImprovement:")
        print(f"  Metrics before: {baseline_biased_count}")
        print(f"  Metrics after: {mitigated_biased_count}")
        print(f"  Overall improvement: {improvement}")

        # Calculate detailed metric improvements
        metric_improvements = _calculate_metric_improvements(baseline_bias, mitigated_bias)

        # Clean results for JSON
        _serialize_mitigation_result(pipeline_results)

        response = {
            "status": "success",
            "pipeline_config": {
                "strategies": strategies,
                "sensitive_feature": sensitive_feature,
                "total_stages": len(strategies),
            },
            "pipeline_execution": {
                "stages": pipeline_stages,
                "total_stages_executed": len([s for s in pipeline_stages if s.get("status") in ["success", "partial"]]),
            },
            "bias_assessment": {
                "baseline": {
                    "summary": baseline_bias["summary"],
                    "detailed_metrics": _serialize_mitigation_result(
                        {k: v for k, v in baseline_bias.items() if k != "summary"}
                    ),
                },
                "after_pipeline": {
                    "summary": mitigated_bias["summary"],
                    "detailed_metrics": _serialize_mitigation_result(
                        {k: v for k, v in mitigated_bias.items() if k != "summary"}
                    ),
                },
            },
            "improvement_analysis": {
                "baseline_biased_metrics_count": int(baseline_biased_count),
                "mitigated_biased_metrics_count": int(mitigated_biased_count),
                "overall_improvement": int(improvement),
                "improvement_percentage": float(
                    (improvement / baseline_biased_count * 100) if baseline_biased_count > 0 else 0
                ),
                "status": "IMPROVED" if improvement > 0 else ("WORSENED" if improvement < 0 else "NO_CHANGE"),
                "metric_improvements": metric_improvements,
            },
            "recommendations": _generate_mitigation_recommendations(
                improvement, "pipeline", mitigated_bias["summary"]["biased_metrics"]
            ),
        }

        return response

    except HTTPException:
        raise
    except Exception as e:
        print(f"[ERROR] Error applying pipeline: {str(e)}")
        import traceback

        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error applying pipeline: {str(e)}")


@router.post("/pipeline/compare")
async def compare_mitigation_pipelines(
    request: PipelineComparisonRequest, current_user: str = Depends(get_current_user), project_id: str = None
):
    """
    Compare multiple mitigation pipelines
    Each pipeline can have different combinations of pre, in, and post-processing strategies
    """
    token = current_user.get("token")
    if not project_id:
        raise HTTPException(status_code=400, detail="project_id is required")

    try:
        # Load data using shared helper
        train_df, X_train, X_train_encoded, y_true, sensitive_attr, model, y_pred = _load_and_prepare(
            token=token,
            project_id=project_id,
            target_column=request.target_column,
            prediction_column=request.prediction_column,
            sensitive_feature_column=request.sensitive_feature_column,
            require_model=True,
        )

        print(f"\n[SENSITIVE FEATURE ANALYSIS]")
        print(f"  Column: {request.sensitive_feature_column}")
        print(f"  Unique values: {np.unique(sensitive_attr)}")
        print(f"  Number of groups: {len(np.unique(sensitive_attr))}")
        for group in np.unique(sensitive_attr):
            count = np.sum(sensitive_attr == group)
            pct = (count / len(sensitive_attr)) * 100
            print(f"  Group {group}: {count} samples ({pct:.1f}%)")

        # Calculate baseline

        bias_detector = BiasDetector()
        baseline_bias = bias_detector.run_all_metrics(y_true, y_pred, sensitive_attr)
        baseline_bias = _clean_nan_values(baseline_bias)
        baseline_biased_count = baseline_bias["summary"]["biased_metrics_count"]

        print(f"\n{'='*60}")
        print(f"COMPARING {len(request.pipelines)} MITIGATION PIPELINES")
        print(f"{'='*60}")
        print(f"Baseline biased metrics: {baseline_biased_count}")

        # Prepare data
        X_train = train_df.drop(columns=[request.target_column]).copy()
        X_train_encoded, encoders = _encode_dataframe(X_train)

        # Compare each pipeline
        pipeline_comparisons = {
            "baseline": {
                "name": "Original (no mitigation)",
                "bias_metrics": baseline_bias,
                "biased_metrics_count": baseline_biased_count,
                "improvement": 0,
            }
        }

        for idx, pipeline_config in enumerate(request.pipelines, 1):
            pipeline_name = pipeline_config.get("name", f"Pipeline_{idx}")
            pipeline_strategies = pipeline_config.get("pipeline", {})

            print(f"\n[{idx}/{len(request.pipelines)}] Evaluating: {pipeline_name}")
            print(f"  Pre: {pipeline_strategies.get('pre', [])}")
            print(f"  In: {pipeline_strategies.get('in', [])}")
            print(f"  Post: {pipeline_strategies.get('post', [])}")

            try:
                # Build pipeline
                pipeline = MitigationPipeline()
                for stage in ["pre", "in", "post"]:
                    if stage in pipeline_strategies:
                        for strategy in pipeline_strategies[stage]:
                            pipeline.add_strategy(strategy, stage)

                # Execute pipeline
                pipeline_results = pipeline.execute_pipeline(
                    X_train_encoded, y_true, sensitive_attr, X_test=X_train_encoded, y_test=y_true, y_pred_test=y_pred
                )

                # Generate mitigated predictions
                y_pred_mitigated = y_pred.copy()
                for stage_name, stage_results in pipeline_results["stage_results"].items():
                    for result in stage_results:
                        if result["status"] == "success":
                            strategy_name = result["strategy"]
                            strategy_type = result["result"].get("type", "post-processing")
                            y_pred_mitigated = _apply_mitigation_adjustment(
                                y_pred_mitigated, sensitive_attr, strategy_type, strategy_name
                            )

                # Calculate mitigated bias
                mitigated_bias = bias_detector.run_all_metrics(y_true, y_pred_mitigated, sensitive_attr)
                mitigated_bias = _clean_nan_values(mitigated_bias)
                mitigated_biased_count = mitigated_bias["summary"]["biased_metrics_count"]
                improvement = baseline_biased_count - mitigated_biased_count

                print(f"  Result: {mitigated_biased_count} biased metrics (improvement: {improvement})")

                pipeline_comparisons[pipeline_name] = {
                    "name": pipeline_name,
                    "pipeline_config": pipeline.get_pipeline_summary(),
                    "bias_metrics": mitigated_bias,
                    "biased_metrics_count": int(mitigated_biased_count),
                    "improvement": int(improvement),
                    "status": "success",
                }

            except Exception as e:
                print(f"  Error: {str(e)}")
                pipeline_comparisons[pipeline_name] = {"name": pipeline_name, "status": "error", "error": str(e)}

        # Rank pipelines by improvement
        successful_pipelines = [
            (name, data)
            for name, data in pipeline_comparisons.items()
            if name != "baseline" and data.get("status") == "success"
        ]
        successful_pipelines.sort(key=lambda x: x[1]["improvement"], reverse=True)

        best_pipeline = successful_pipelines[0] if successful_pipelines else None

        print(f"\n{'='*60}")
        print(f"BEST PIPELINE: {best_pipeline[0] if best_pipeline else 'None'}")
        if best_pipeline:
            print(f"Improvement: {best_pipeline[1]['improvement']} fewer biased metrics")
        print(f"{'='*60}\n")

        return {
            "status": "success",
            "analysis_type": "Multi-Pipeline Comparison",
            "sensitive_feature": request.sensitive_feature_column,
            "total_pipelines": len(request.pipelines),
            "baseline_biased_metrics": int(baseline_biased_count),
            "pipeline_comparisons": pipeline_comparisons,
            "best_pipeline": best_pipeline[0] if best_pipeline else None,
            "rankings": [{"pipeline": name, "improvement": data["improvement"]} for name, data in successful_pipelines],
        }

    except HTTPException:
        raise
    except Exception as e:
        print(f"[ERROR] Error comparing pipelines: {str(e)}")
        import traceback

        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error comparing pipelines: {str(e)}")


@router.get("/pipeline/templates")
async def get_pipeline_templates(current_user: str = Depends(get_current_user), project_id: str = None):
    """Get pre-configured pipeline templates for common use cases"""
    return {
        "status": "success",
        "templates": {
            "basic_preprocessing": {
                "name": "Basic Pre-processing",
                "pipeline": {"pre": ["reweighing"], "in": [], "post": []},
                "description": "Simple reweighing for data balance",
            },
            "comprehensive": {
                "name": "Comprehensive Multi-Stage",
                "pipeline": {
                    "pre": ["reweighing", "data_augmentation"],
                    "in": ["fairness_regularization"],
                    "post": ["threshold_optimization"],
                },
                "description": "Full pipeline with all stages",
            },
            "post_processing_only": {
                "name": "Post-Processing Only",
                "pipeline": {"pre": [], "in": [], "post": ["calibration_adjustment", "equalized_odds_postprocessing"]},
                "description": "No retraining needed - only prediction adjustments",
            },
            "data_centric": {
                "name": "Data-Centric Approach",
                "pipeline": {"pre": ["data_augmentation", "disparate_impact_remover"], "in": [], "post": []},
                "description": "Focus on data preparation",
            },
            "model_centric": {
                "name": "Model-Centric Approach",
                "pipeline": {"pre": [], "in": ["fairness_regularization", "adversarial_debiasing"], "post": []},
                "description": "Focus on fair model training",
            },
        },
    }


@router.post("/optimize")
async def optimize_pipeline(
    request: OptimizationRequest, current_user: str = Depends(get_current_user), project_id: str = None
):
    """
    Automatically find the best mitigation pipeline using specified optimization method

    Methods:
    - greedy: Greedy search (fast, good results)
    - top_k: Top-K method (moderate speed, good results)
    - brute_force: Exhaustive search (slow, optimal results)
    """
    token = current_user.get("token")
    if not project_id:
        raise HTTPException(status_code=400, detail="project_id is required")

    try:
        # Load & prepare using shared helper
        train_df, X_train, X_train_encoded, y_true, sensitive_attr, model, y_pred = _load_and_prepare(
            token=token,
            project_id=project_id,
            target_column=request.target_column,
            prediction_column=request.prediction_column,
            sensitive_feature_column=request.sensitive_feature_column,
            require_model=True,
        )

        print(f"\n[SENSITIVE FEATURE ANALYSIS]")
        print(f"  Column: {request.sensitive_feature_column}")
        print(f"  Unique values: {np.unique(sensitive_attr)}")
        print(f"  Number of groups: {len(np.unique(sensitive_attr))}")

        # Validate model is available for optimization
        if model is None:
            raise HTTPException(
                status_code=400,
                detail="Model is required for optimization (upload model or ensure it was saved). Strategies like fairness_regularization and threshold_optimization require a model.",
            )

        # Calculate baseline
        bias_detector = BiasDetector()
        baseline_bias = bias_detector.run_all_metrics(y_true, y_pred, sensitive_attr)
        baseline_bias = _clean_nan_values(baseline_bias)
        baseline_biased_count = baseline_bias["summary"]["biased_metrics_count"]

        # Proceed to optimization...
        # Run optimization
        from services.fairness.app.src.pipeline_optimizer import PipelineOptimizer

        optimizer = PipelineOptimizer(bias_detector=bias_detector, user_model=model)

        if request.method == "greedy":
            result = optimizer.stage_wise_greedy_search(
                X_train_encoded, y_true, sensitive_attr, y_pred, baseline_biased_count
            )

        elif request.method == "top_k":
            result = optimizer.top_k_method(
                X_train_encoded, y_true, sensitive_attr, y_pred, baseline_biased_count, k=request.k or 5
            )

        elif request.method == "brute_force":
            result = optimizer.brute_force_search(
                X_train_encoded,
                y_true,
                sensitive_attr,
                y_pred,
                baseline_biased_count,
                max_strategies_per_stage=request.max_strategies_per_stage or 2,
            )

        else:
            raise HTTPException(status_code=400, detail=f"Unknown optimization method: {request.method}")

        # Clean for JSON
        result_clean = _serialize_mitigation_result(result)

        return {
            "status": "success",
            "optimization_method": request.method,
            "baseline_biased_metrics": int(baseline_biased_count),
            "optimization_result": result_clean,
            "recommended_pipeline": result["best_pipeline"],
        }

    except HTTPException:
        raise
    except Exception as e:
        print(f"[ERROR] Error in optimization: {str(e)}")
        import traceback

        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error in optimization: {str(e)}")


@router.post("/optimize/compare-methods")
async def compare_optimization_methods(
    request: BaseModel, current_user: str = Depends(get_current_user), project_id: str = None
):
    """
    Compare all three optimization methods on the same dataset
    Shows which method finds the best pipeline and how long each takes
    """
    # Implementation similar to above, runs all three methods


def _generate_comparison_summary(mitigation_results: Dict) -> Dict:
    """Generate summary comparing strategies"""
    summary = {
        "baseline_biased_metrics": mitigation_results["baseline"]["bias_metrics"]
        .get("summary", {})
        .get("biased_metrics_count", 0),
        "strategies_evaluated": len(mitigation_results) - 1,
        "best_strategies": [],
        "improvement_summary": {},
    }

    baseline_biased_count = summary["baseline_biased_metrics"]

    for strategy_name, result in mitigation_results.items():
        if strategy_name == "baseline" or result.get("status") == "error":
            continue

        try:
            strategy_biased_count = result["bias_metrics"].get("summary", {}).get("biased_metrics_count", 0)
            improvement = baseline_biased_count - strategy_biased_count

            summary["improvement_summary"][strategy_name] = {
                "biased_metrics_before": baseline_biased_count,
                "biased_metrics_after": strategy_biased_count,
                "improvement": improvement,
                "type": result.get("strategy_info", {}).get("type", "unknown"),
            }

            if improvement > 0:
                summary["best_strategies"].append({"strategy": strategy_name, "improvement": improvement})
        except:
            pass

    # Sort best strategies by improvement
    summary["best_strategies"].sort(key=lambda x: x["improvement"], reverse=True)

    return summary


def _serialize_mitigation_result(result: Dict) -> Dict:
    """
    Convert non-JSON-serializable objects to JSON-compatible format
    """
    serialized = {}

    for key, value in result.items():
        if isinstance(value, np.ndarray):
            # Convert numpy arrays to lists
            serialized[key] = value.tolist() if len(value.shape) > 0 else float(value)
        elif isinstance(value, (np.int32, np.int64)):
            serialized[key] = int(value)
        elif isinstance(value, (np.float32, np.float64)):
            serialized[key] = float(value)
        elif isinstance(value, np.bool_):
            serialized[key] = bool(value)
        elif isinstance(value, dict):
            serialized[key] = _serialize_mitigation_result(value)
        elif isinstance(value, list):
            serialized[key] = [
                (
                    _serialize_mitigation_result(item)
                    if isinstance(item, dict)
                    else (item.tolist() if isinstance(item, np.ndarray) else item)
                )
                for item in value
            ]
        elif value is None:
            serialized[key] = None
        else:
            serialized[key] = str(value)

    return serialized


def _apply_mitigation_adjustment(
    y_pred: np.ndarray,
    sensitive_attr: np.ndarray,
    strategy_type: str,
    strategy_name: str,
    y_true: np.ndarray = None,
    X_transformed: np.ndarray = None,
    model=None,
) -> np.ndarray:
    """
    Apply prediction adjustments based on mitigation strategy
    """
    y_pred_adjusted = y_pred.copy().astype(float)

    if strategy_type == "pre-processing":
        print(f"  Pre-processing strategy detected")

        # If we have transformed data and model, use it for new predictions
        if X_transformed is not None and model is not None:
            print(f"    Using model to predict on transformed data...")
            y_pred_adjusted = model.predict(X_transformed).astype(float)

        # Otherwise, apply fairness-aware adjustments to existing predictions
        elif strategy_name == "disparate_impact_remover":
            print(f"    Applying disparate impact adjustments...")
            # Flip predictions for underrepresented groups to balance outcomes
            for group in np.unique(sensitive_attr):
                group_mask = sensitive_attr == group
                group_positive_rate = np.mean(y_pred[group_mask])

                # If this group has very low positive rate, increase it slightly
                if group_positive_rate < 0.3:
                    flip_indices = np.where(group_mask & (y_pred == 0))[0]
                    if len(flip_indices) > 0:
                        num_flips = int(len(flip_indices) * 0.15)  # Flip 15%
                        flip_idx = np.random.choice(flip_indices, size=num_flips, replace=False)
                        y_pred_adjusted[flip_idx] = 1
                        print(f"      Group {group}: flipped {num_flips} predictions")

        elif strategy_name == "data_augmentation":
            print(f"    Applying data augmentation adjustments...")
            # Balanced representation - adjust predictions to reflect balance

        return y_pred_adjusted.astype(int)

    elif strategy_type == "in-processing":
        print(f"  In-processing strategy detected")
        # Model was already fine-tuned, predictions remain same
        return y_pred_adjusted.astype(int)

    elif strategy_type == "post-processing":
        print(f"  Post-processing strategy detected - applying threshold adjustments")

        if strategy_name == "threshold_optimization":
            # Adjust thresholds per group
            for group in np.unique(sensitive_attr):
                group_mask = sensitive_attr == group
                group_pred = y_pred[group_mask]

                threshold = np.percentile(group_pred, 50)
                y_pred_adjusted[group_mask] = (group_pred > threshold).astype(float)
                print(f"      Group {group}: threshold = {threshold:.3f}")

        elif strategy_name == "calibration_adjustment":
            # Smooth predictions
            y_pred_adjusted = 0.9 * y_pred_adjusted + 0.1 * np.random.random(len(y_pred))
            y_pred_adjusted = np.clip(y_pred_adjusted, 0, 1)

        return y_pred_adjusted.astype(int)

    return y_pred_adjusted.astype(int)


def _calculate_metric_improvements(baseline_bias: Dict, mitigated_bias: Dict) -> Dict:
    """
    Calculate improvements for each individual metric
    """
    improvements = {}

    # Extract metrics from both
    baseline_metrics = {k: v for k, v in baseline_bias.items() if k != "summary"}
    mitigated_metrics = {k: v for k, v in mitigated_bias.items() if k != "summary"}

    for metric_name in baseline_metrics.keys():
        if metric_name in mitigated_metrics:
            baseline_metric = baseline_metrics[metric_name]
            mitigated_metric = mitigated_metrics[metric_name]

            if isinstance(baseline_metric, dict) and "is_biased" in baseline_metric:
                was_biased = baseline_metric.get("is_biased", False)
                is_now_biased = mitigated_metric.get("is_biased", False)

                if was_biased and not is_now_biased:
                    improvements[metric_name] = "FIXED"
                elif not was_biased and is_now_biased:
                    improvements[metric_name] = "BROKEN"
                else:
                    improvements[metric_name] = "NO_CHANGE"

    return improvements


def _generate_mitigation_recommendations(
    improvement: int, strategy_type: str, remaining_biased_metrics: List[str]
) -> List[str]:
    """
    Generate recommendations based on mitigation results
    """
    recommendations = []

    if improvement > 0:
        recommendations.append(f"âœ… Strategy successfully reduced biased metrics by {improvement}")
    elif improvement < 0:
        recommendations.append(f"âŒ Strategy increased biased metrics by {abs(improvement)}")
    else:
        recommendations.append("âž¡ï¸ Strategy had no impact on overall bias")

    if remaining_biased_metrics:
        recommendations.append(f"âš ï¸ Remaining biased metrics: {', '.join(remaining_biased_metrics)}")
        recommendations.append("ðŸ’¡ Consider combining this strategy with others for better results")
    else:
        recommendations.append("âœ¨ All bias metrics resolved!")

    if strategy_type == "pre-processing":
        recommendations.append("ðŸ“Š Pre-processing strategy applied - retrain model for best results")
    elif strategy_type == "post-processing":
        recommendations.append("ðŸŽ¯ Post-processing strategy applied - no model retraining needed")

    return recommendations
